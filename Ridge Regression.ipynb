{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0   1   2   3     4     5     6     7     8     9     10    11   12  \\\n",
      "0   Index  A1  A2  A3    A4    A5    A6    A7    A8    A9   A10   A11  A12   \n",
      "1       1  36  27  71   8.1  3.34  11.4  81.5  3243   8.8  42.6  11.7   21   \n",
      "2       2  35  23  72  11.1  3.14    11  78.8  4281   3.6  50.7  14.4    8   \n",
      "3       3  44  29  74  10.4  3.21   9.8  81.6  4260   0.8  39.4  12.4    6   \n",
      "4       4  47  45  79   6.5  3.41  11.1  77.5  3125  27.1  50.2  20.6   18   \n",
      "..    ...  ..  ..  ..   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
      "56     56  28  32  81     7  3.27  12.1    81  3665   7.5  51.6  13.2    4   \n",
      "57     57  45  33  76   7.7  3.39  11.3  82.2  3152  12.1  47.3  10.9   14   \n",
      "58     58  45  24  70  11.8  3.25  11.1  79.8  3678     1  44.8    14    7   \n",
      "59     59  42  83  76   9.7  3.22     9  76.2  9699   4.8  42.2  14.5    8   \n",
      "60     60  38  28  72   8.9  3.48  10.7  79.8  3451  11.7  37.5    13   14   \n",
      "\n",
      "     13   14   15       16  \n",
      "0   A13  A14  A15        B  \n",
      "1    15   59   59   921.87  \n",
      "2    10   39   57  997.875  \n",
      "3     6   33   54  962.354  \n",
      "4     8   24   56  982.291  \n",
      "..  ...  ...  ...      ...  \n",
      "56    2    1   54  823.764  \n",
      "57   11   42   56   1003.5  \n",
      "58    3    8   56  895.696  \n",
      "59    8   49   54  911.817  \n",
      "60   13   39   58  954.442  \n",
      "\n",
      "[61 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "xl = pd.ExcelFile(r'C:\\Users\\ASUS ZenBook\\OneDrive\\Desktop\\clfd.xlsx')\n",
    "df = pd.read_excel(xl, 0, header=None)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Index' 'A1' 'A2' ... 'A14' 'A15' 'B']\n",
      " [1 36 27 ... 59 59 921.87]\n",
      " [2 35 23 ... 39 57 997.875]\n",
      " ...\n",
      " [58 45 24 ... 8 56 895.696]\n",
      " [59 42 83 ... 49 54 911.817]\n",
      " [60 38 28 ... 39 58 954.442]]\n"
     ]
    }
   ],
   "source": [
    "A=np.array(df)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.600e+01 2.700e+01 7.100e+01 8.100e+00 3.340e+00 1.140e+01 8.150e+01\n",
      "  3.243e+03 8.800e+00 4.260e+01 1.170e+01 2.100e+01 1.500e+01 5.900e+01\n",
      "  5.900e+01]\n",
      " [3.500e+01 2.300e+01 7.200e+01 1.110e+01 3.140e+00 1.100e+01 7.880e+01\n",
      "  4.281e+03 3.600e+00 5.070e+01 1.440e+01 8.000e+00 1.000e+01 3.900e+01\n",
      "  5.700e+01]\n",
      " [4.400e+01 2.900e+01 7.400e+01 1.040e+01 3.210e+00 9.800e+00 8.160e+01\n",
      "  4.260e+03 8.000e-01 3.940e+01 1.240e+01 6.000e+00 6.000e+00 3.300e+01\n",
      "  5.400e+01]\n",
      " [4.700e+01 4.500e+01 7.900e+01 6.500e+00 3.410e+00 1.110e+01 7.750e+01\n",
      "  3.125e+03 2.710e+01 5.020e+01 2.060e+01 1.800e+01 8.000e+00 2.400e+01\n",
      "  5.600e+01]\n",
      " [4.300e+01 3.500e+01 7.700e+01 7.600e+00 3.440e+00 9.600e+00 8.460e+01\n",
      "  6.441e+03 2.440e+01 4.370e+01 1.430e+01 4.300e+01 3.800e+01 2.060e+02\n",
      "  5.500e+01]\n",
      " [5.300e+01 4.500e+01 8.000e+01 7.700e+00 3.450e+00 1.020e+01 6.680e+01\n",
      "  3.325e+03 3.850e+01 4.310e+01 2.550e+01 3.000e+01 3.200e+01 7.200e+01\n",
      "  5.400e+01]\n",
      " [4.300e+01 3.000e+01 7.400e+01 1.090e+01 3.230e+00 1.210e+01 8.390e+01\n",
      "  4.679e+03 3.500e+00 4.920e+01 1.130e+01 2.100e+01 3.200e+01 6.200e+01\n",
      "  5.600e+01]\n",
      " [4.500e+01 3.000e+01 7.300e+01 9.300e+00 3.290e+00 1.060e+01 8.600e+01\n",
      "  2.140e+03 5.300e+00 4.040e+01 1.050e+01 6.000e+00 4.000e+00 4.000e+00\n",
      "  5.600e+01]\n",
      " [3.600e+01 2.400e+01 7.000e+01 9.000e+00 3.310e+00 1.050e+01 8.320e+01\n",
      "  6.582e+03 8.100e+00 4.250e+01 1.260e+01 1.800e+01 1.200e+01 3.700e+01\n",
      "  6.100e+01]\n",
      " [3.600e+01 2.700e+01 7.200e+01 9.500e+00 3.360e+00 1.070e+01 7.930e+01\n",
      "  4.213e+03 6.700e+00 4.100e+01 1.320e+01 1.200e+01 7.000e+00 2.000e+01\n",
      "  5.900e+01]\n",
      " [5.200e+01 4.200e+01 7.900e+01 7.700e+00 3.390e+00 9.600e+00 6.920e+01\n",
      "  2.302e+03 2.220e+01 4.130e+01 2.420e+01 1.800e+01 8.000e+00 2.700e+01\n",
      "  5.600e+01]\n",
      " [3.300e+01 2.600e+01 7.600e+01 8.600e+00 3.200e+00 1.090e+01 8.340e+01\n",
      "  6.122e+03 1.630e+01 4.490e+01 1.070e+01 8.800e+01 6.300e+01 2.780e+02\n",
      "  5.800e+01]\n",
      " [4.000e+01 3.400e+01 7.700e+01 9.200e+00 3.210e+00 1.020e+01 7.700e+01\n",
      "  4.101e+03 1.300e+01 4.570e+01 1.510e+01 2.600e+01 2.600e+01 1.460e+02\n",
      "  5.700e+01]\n",
      " [3.500e+01 2.800e+01 7.100e+01 8.800e+00 3.290e+00 1.110e+01 8.630e+01\n",
      "  3.042e+03 1.470e+01 4.460e+01 1.140e+01 3.100e+01 2.100e+01 6.400e+01\n",
      "  6.000e+01]\n",
      " [3.700e+01 3.100e+01 7.500e+01 8.000e+00 3.260e+00 1.190e+01 7.840e+01\n",
      "  4.259e+03 1.310e+01 4.960e+01 1.390e+01 2.300e+01 9.000e+00 1.500e+01\n",
      "  5.800e+01]\n",
      " [3.500e+01 4.600e+01 8.500e+01 7.100e+00 3.220e+00 1.180e+01 7.990e+01\n",
      "  1.441e+03 1.480e+01 5.120e+01 1.610e+01 1.000e+00 1.000e+00 1.000e+00\n",
      "  5.400e+01]\n",
      " [3.600e+01 3.000e+01 7.500e+01 7.500e+00 3.350e+00 1.140e+01 8.190e+01\n",
      "  4.029e+03 1.240e+01 4.400e+01 1.200e+01 6.000e+00 4.000e+00 1.600e+01\n",
      "  5.800e+01]\n",
      " [1.500e+01 3.000e+01 7.300e+01 8.200e+00 3.150e+00 1.220e+01 8.420e+01\n",
      "  4.824e+03 4.700e+00 5.310e+01 1.270e+01 1.700e+01 8.000e+00 2.800e+01\n",
      "  3.800e+01]\n",
      " [3.100e+01 2.700e+01 7.400e+01 7.200e+00 3.440e+00 1.080e+01 8.700e+01\n",
      "  4.834e+03 1.580e+01 4.350e+01 1.360e+01 5.200e+01 3.500e+01 1.240e+02\n",
      "  5.900e+01]\n",
      " [3.000e+01 2.400e+01 7.200e+01 6.500e+00 3.530e+00 1.080e+01 7.950e+01\n",
      "  3.694e+03 1.310e+01 3.380e+01 1.240e+01 1.100e+01 4.000e+00 1.100e+01\n",
      "  6.100e+01]\n",
      " [3.100e+01 4.500e+01 8.500e+01 7.300e+00 3.220e+00 1.140e+01 8.070e+01\n",
      "  1.844e+03 1.150e+01 4.810e+01 1.850e+01 1.000e+00 1.000e+00 1.000e+00\n",
      "  5.300e+01]\n",
      " [3.100e+01 2.400e+01 7.200e+01 9.000e+00 3.370e+00 1.090e+01 8.280e+01\n",
      "  3.226e+03 5.100e+00 4.520e+01 1.230e+01 5.000e+00 3.000e+00 1.000e+01\n",
      "  6.100e+01]\n",
      " [4.200e+01 4.000e+01 7.700e+01 6.100e+00 3.450e+00 1.040e+01 7.180e+01\n",
      "  2.269e+03 2.270e+01 4.140e+01 1.950e+01 8.000e+00 3.000e+00 5.000e+00\n",
      "  5.300e+01]\n",
      " [4.300e+01 2.700e+01 7.200e+01 9.000e+00 3.250e+00 1.150e+01 8.710e+01\n",
      "  2.909e+03 7.200e+00 5.160e+01 9.500e+00 7.000e+00 3.000e+00 1.000e+01\n",
      "  5.600e+01]\n",
      " [4.600e+01 5.500e+01 8.400e+01 5.600e+00 3.350e+00 1.140e+01 7.970e+01\n",
      "  2.647e+03 2.100e+01 4.690e+01 1.790e+01 6.000e+00 5.000e+00 1.000e+00\n",
      "  5.900e+01]\n",
      " [3.900e+01 2.900e+01 7.600e+01 8.700e+00 3.230e+00 1.140e+01 7.860e+01\n",
      "  4.412e+03 1.560e+01 4.660e+01 1.320e+01 1.300e+01 7.000e+00 3.300e+01\n",
      "  6.000e+01]\n",
      " [3.500e+01 3.100e+01 8.100e+01 9.200e+00 3.100e+00 1.200e+01 7.830e+01\n",
      "  3.262e+03 1.260e+01 4.860e+01 1.390e+01 7.000e+00 4.000e+00 4.000e+00\n",
      "  5.500e+01]\n",
      " [4.300e+01 3.200e+01 7.400e+01 1.010e+01 3.380e+00 9.500e+00 7.920e+01\n",
      "  3.214e+03 2.900e+00 4.370e+01 1.200e+01 1.100e+01 7.000e+00 3.200e+01\n",
      "  5.400e+01]\n",
      " [1.100e+01 5.300e+01 6.800e+01 9.200e+00 2.990e+00 1.210e+01 9.060e+01\n",
      "  4.700e+03 7.800e+00 4.890e+01 1.230e+01 6.480e+02 3.190e+02 1.300e+02\n",
      "  4.700e+01]\n",
      " [3.000e+01 3.500e+01 7.100e+01 8.300e+00 3.370e+00 9.900e+00 7.740e+01\n",
      "  4.474e+03 1.310e+01 4.260e+01 1.770e+01 3.800e+01 3.700e+01 1.930e+02\n",
      "  5.700e+01]\n",
      " [5.000e+01 4.200e+01 8.200e+01 7.300e+00 3.490e+00 1.040e+01 7.250e+01\n",
      "  3.497e+03 3.670e+01 4.330e+01 2.640e+01 1.500e+01 1.000e+01 3.400e+01\n",
      "  5.900e+01]\n",
      " [6.000e+01 6.700e+01 8.200e+01 1.000e+01 2.980e+00 1.150e+01 8.860e+01\n",
      "  4.657e+03 1.360e+01 4.730e+01 2.240e+01 3.000e+00 1.000e+00 1.000e+00\n",
      "  6.000e+01]\n",
      " [3.000e+01 2.000e+01 6.900e+01 8.800e+00 3.260e+00 1.110e+01 8.540e+01\n",
      "  2.934e+03 5.800e+00 4.400e+01 9.400e+00 3.300e+01 2.300e+01 1.250e+02\n",
      "  6.400e+01]\n",
      " [2.500e+01 1.200e+01 7.300e+01 9.200e+00 3.280e+00 1.210e+01 8.310e+01\n",
      "  2.095e+03 2.000e+00 5.190e+01 9.800e+00 2.000e+01 1.100e+01 2.600e+01\n",
      "  5.000e+01]\n",
      " [4.500e+01 4.000e+01 8.000e+01 8.300e+00 3.320e+00 1.010e+01 7.030e+01\n",
      "  2.682e+03 2.100e+01 4.610e+01 2.410e+01 1.700e+01 1.400e+01 7.800e+01\n",
      "  5.600e+01]\n",
      " [4.600e+01 3.000e+01 7.200e+01 1.020e+01 3.160e+00 1.130e+01 8.320e+01\n",
      "  3.327e+03 8.800e+00 4.530e+01 1.220e+01 4.000e+00 3.000e+00 8.000e+00\n",
      "  5.800e+01]\n",
      " [5.400e+01 5.400e+01 8.100e+01 7.400e+00 3.360e+00 9.700e+00 7.280e+01\n",
      "  3.172e+03 3.140e+01 4.550e+01 2.420e+01 2.000e+01 1.700e+01 1.000e+00\n",
      "  6.200e+01]\n",
      " [4.200e+01 3.300e+01 7.700e+01 9.700e+00 3.030e+00 1.070e+01 8.350e+01\n",
      "  7.462e+03 1.130e+01 4.870e+01 1.240e+01 4.100e+01 2.600e+01 1.080e+02\n",
      "  5.800e+01]\n",
      " [4.200e+01 3.200e+01 7.600e+01 9.100e+00 3.320e+00 1.050e+01 8.750e+01\n",
      "  6.092e+03 1.750e+01 4.530e+01 1.320e+01 2.900e+01 3.200e+01 1.610e+02\n",
      "  5.400e+01]\n",
      " [3.600e+01 2.900e+01 7.200e+01 9.500e+00 3.320e+00 1.060e+01 7.760e+01\n",
      "  3.437e+03 8.100e+00 4.550e+01 1.380e+01 4.500e+01 5.900e+01 2.630e+02\n",
      "  5.600e+01]\n",
      " [3.700e+01 3.800e+01 6.700e+01 1.130e+01 2.990e+00 1.200e+01 8.150e+01\n",
      "  3.387e+03 3.600e+00 5.030e+01 1.350e+01 5.600e+01 2.100e+01 4.400e+01\n",
      "  7.300e+01]\n",
      " [4.200e+01 2.900e+01 7.200e+01 1.070e+01 3.190e+00 1.010e+01 7.950e+01\n",
      "  3.508e+03 2.200e+00 3.830e+01 1.570e+01 6.000e+00 4.000e+00 1.800e+01\n",
      "  5.600e+01]\n",
      " [4.100e+01 3.300e+01 7.700e+01 1.120e+01 3.080e+00 9.600e+00 7.990e+01\n",
      "  4.843e+03 2.700e+00 3.860e+01 1.410e+01 1.100e+01 1.100e+01 8.900e+01\n",
      "  5.400e+01]\n",
      " [4.400e+01 3.900e+01 7.800e+01 8.200e+00 3.320e+00 1.100e+01 7.990e+01\n",
      "  3.768e+03 2.860e+01 4.950e+01 1.750e+01 1.200e+01 9.000e+00 4.800e+01\n",
      "  5.300e+01]\n",
      " [3.200e+01 2.500e+01 7.200e+01 1.090e+01 3.210e+00 1.110e+01 8.250e+01\n",
      "  4.355e+03 5.000e+00 4.640e+01 1.080e+01 7.000e+00 4.000e+00 1.800e+01\n",
      "  6.000e+01]\n",
      " [3.400e+01 3.200e+01 7.900e+01 9.300e+00 3.230e+00 9.700e+00 7.680e+01\n",
      "  5.160e+03 1.720e+01 4.510e+01 1.530e+01 3.100e+01 1.500e+01 6.800e+01\n",
      "  5.700e+01]\n",
      " [1.000e+01 5.500e+01 7.000e+01 7.300e+00 3.110e+00 1.210e+01 8.890e+01\n",
      "  3.033e+03 5.900e+00 5.100e+01 1.400e+01 1.440e+02 6.600e+01 2.000e+01\n",
      "  6.100e+01]\n",
      " [1.800e+01 4.800e+01 6.300e+01 9.200e+00 2.920e+00 1.220e+01 8.770e+01\n",
      "  4.253e+03 1.370e+01 5.120e+01 1.200e+01 3.110e+02 1.710e+02 8.600e+01\n",
      "  7.100e+01]\n",
      " [1.300e+01 4.900e+01 6.800e+01 7.000e+00 3.360e+00 1.220e+01 9.070e+01\n",
      "  2.702e+03 3.000e+00 5.190e+01 9.700e+00 1.050e+02 3.200e+01 3.000e+00\n",
      "  7.100e+01]\n",
      " [3.500e+01 4.000e+01 6.400e+01 9.600e+00 3.020e+00 1.220e+01 8.250e+01\n",
      "  3.626e+03 5.700e+00 5.430e+01 1.010e+01 2.000e+01 7.000e+00 2.000e+01\n",
      "  7.200e+01]\n",
      " [4.500e+01 2.800e+01 7.400e+01 1.060e+01 3.210e+00 1.110e+01 8.260e+01\n",
      "  1.883e+03 3.400e+00 4.190e+01 1.230e+01 5.000e+00 4.000e+00 2.000e+01\n",
      "  5.600e+01]\n",
      " [3.800e+01 2.400e+01 7.200e+01 9.800e+00 3.340e+00 1.140e+01 7.800e+01\n",
      "  4.923e+03 3.800e+00 5.050e+01 1.110e+01 8.000e+00 5.000e+00 2.500e+01\n",
      "  6.100e+01]\n",
      " [3.100e+01 2.600e+01 7.300e+01 9.300e+00 3.220e+00 1.070e+01 8.130e+01\n",
      "  3.249e+03 9.500e+00 4.390e+01 1.360e+01 1.100e+01 7.000e+00 2.500e+01\n",
      "  5.900e+01]\n",
      " [4.000e+01 2.300e+01 7.100e+01 1.130e+01 3.280e+00 1.030e+01 7.380e+01\n",
      "  1.671e+03 2.500e+00 4.740e+01 1.350e+01 5.000e+00 2.000e+00 1.100e+01\n",
      "  6.000e+01]\n",
      " [4.100e+01 3.700e+01 7.800e+01 6.200e+00 3.250e+00 1.230e+01 8.950e+01\n",
      "  5.308e+03 2.590e+01 5.970e+01 1.030e+01 6.500e+01 2.800e+01 1.020e+02\n",
      "  5.200e+01]\n",
      " [2.800e+01 3.200e+01 8.100e+01 7.000e+00 3.270e+00 1.210e+01 8.100e+01\n",
      "  3.665e+03 7.500e+00 5.160e+01 1.320e+01 4.000e+00 2.000e+00 1.000e+00\n",
      "  5.400e+01]\n",
      " [4.500e+01 3.300e+01 7.600e+01 7.700e+00 3.390e+00 1.130e+01 8.220e+01\n",
      "  3.152e+03 1.210e+01 4.730e+01 1.090e+01 1.400e+01 1.100e+01 4.200e+01\n",
      "  5.600e+01]\n",
      " [4.500e+01 2.400e+01 7.000e+01 1.180e+01 3.250e+00 1.110e+01 7.980e+01\n",
      "  3.678e+03 1.000e+00 4.480e+01 1.400e+01 7.000e+00 3.000e+00 8.000e+00\n",
      "  5.600e+01]\n",
      " [4.200e+01 8.300e+01 7.600e+01 9.700e+00 3.220e+00 9.000e+00 7.620e+01\n",
      "  9.699e+03 4.800e+00 4.220e+01 1.450e+01 8.000e+00 8.000e+00 4.900e+01\n",
      "  5.400e+01]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X=(A[1:60, 1:16])\n",
    "X=np.array(X, dtype='float')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 921.87   997.875  962.354  982.291 1071.289 1030.38   934.7    899.529\n",
      " 1001.902  912.347 1017.613 1024.885  970.467  985.95   958.839  860.101\n",
      "  936.234  871.766  959.221  941.181  891.708  871.338  971.122  887.466\n",
      "  952.529  968.665  919.729  844.053  861.833  989.265 1006.49   861.439\n",
      "  929.15   857.622  961.009  923.234 1113.156  994.648 1015.023  991.29\n",
      "  893.991  938.5    946.185 1025.502  874.281  953.56   839.709  911.701\n",
      "  790.733  899.264  904.155  950.672  972.464  912.202  967.803  823.764\n",
      " 1003.502  895.696  911.817]\n"
     ]
    }
   ],
   "source": [
    "Y=(A[1:60, 16] )\n",
    "Y=np.array(Y, dtype='float')\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LAMBDA: 0.002\n",
      "1592.9379747078963\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "def normalize_and_add_ones(X):\n",
    "    X=np.array(X)\n",
    "    X_max=np.array([[np.amax(X[:, column_id])\n",
    "                    for column_id in range(X.shape[1])]\n",
    "                   for _ in range(X.shape[0])])\n",
    "    X_min=np.array([[np.amin(X[:, column_id])\n",
    "                    for column_id in range(X.shape[1])]\n",
    "                   for _ in range(X.shape[0])])\n",
    "    X_normalized = (X-X_min)/(X_max-X_min)\n",
    "    ones=np.array([[1] for _ in range(X_normalized.shape[0])])\n",
    "    return np.column_stack((ones, X_normalized))\n",
    "class RidgeRegression:\n",
    "    def __init__(self):\n",
    "        return\n",
    "    def fit(self, X_train, Y_train, LAMBDA):\n",
    "        assert len(X_train.shape)==2 and \\\n",
    "               X_train.shape[0]==Y_train.shape[0]\n",
    "        \n",
    "        W=np.linalg.inv(\n",
    "          X_train.transpose().dot(X_train)+LAMBDA*np.identity(X_train.shape[1])\n",
    "        ).dot(X_train. transpose()).dot(Y_train)\n",
    "        return W\n",
    "    def predict(self, W, X_new):\n",
    "        X_new=np.array(X_new)\n",
    "        Y_new=X_new.dot(W)\n",
    "        return Y_new\n",
    "    def compute_RSS(self, Y_new, Y_predicted):\n",
    "        loss=1./Y_new.shape[0]*\\\n",
    "             np.sum((Y_new-Y_predicted)**2)\n",
    "        return loss\n",
    "    def get_the_best_LAMBDA(self, X_train, Y_train):\n",
    "        def cross_validation(num_folds, LAMBDA):\n",
    "            row_ids=np.array(range(X_train.shape[0]))\n",
    "            valid_ids=np.split(row_ids[:len(row_ids)-len(row_ids)%num_folds], num_folds)\n",
    "            valid_ids[-1]=np.append(valid_ids[-1], row_ids[len(row_ids)-len(row_ids)%num_folds:])\n",
    "            train_ids=[[k for k in row_ids if k not in valid_ids[i]] for i in range(num_folds)]\n",
    "            aver_RSS=0\n",
    "            for i in range(num_folds):\n",
    "                valid_part={'X':X_train[valid_ids[i]], 'Y':Y_train[valid_ids[i]]}\n",
    "                train_part={'X':X_train[train_ids[i]], 'Y':Y_train[train_ids[i]]}\n",
    "                W=self.fit(train_part['X'], train_part['Y'], LAMBDA)\n",
    "                Y_predicted=self.predict(W, valid_part['X'])\n",
    "                aver_RSS += self.compute_RSS(valid_part['Y'], Y_predicted)\n",
    "            return aver_RSS/num_folds\n",
    "        def range_scan(best_LAMBDA, minimum_RSS, LAMBDA_values):\n",
    "            for current_LAMBDA in LAMBDA_values:\n",
    "                aver_RSS = cross_validation(num_folds=5, LAMBDA=current_LAMBDA)\n",
    "                if aver_RSS<minimum_RSS:\n",
    "                    best_LAMBDA = current_LAMBDA\n",
    "                    minimum_RSS=aver_RSS\n",
    "            return best_LAMBDA, minimum_RSS\n",
    "        best_LAMBDA, minimum_RSS= range_scan(best_LAMBDA=0, minimum_RSS=1000**2,\n",
    "                                        LAMBDA_values=range(50))\n",
    "        LAMBDA_values=[k*1./1000 for k in range(\n",
    "            max(0,(best_LAMBDA-1)*1000), (best_LAMBDA+1)*1000,1)]\n",
    "        best_LAMBDA, minimum_RSS= range_scan(best_LAMBDA=best_LAMBDA, minimum_RSS=minimum_RSS, LAMBDA_values=LAMBDA_values)\n",
    "        return best_LAMBDA \n",
    "\n",
    "X=normalize_and_add_ones(X)\n",
    "X_train, Y_train =X[:50], Y[:50]\n",
    "X_test, Y_test=X[50:],Y[50:]\n",
    "ridge_regression=RidgeRegression()\n",
    "best_LAMBDA=ridge_regression.get_the_best_LAMBDA(X_train, Y_train)\n",
    "print ('Best LAMBDA:', best_LAMBDA)\n",
    "W_learned=ridge_regression.fit(X_train=X_train, Y_train=Y_train, LAMBDA=best_LAMBDA)\n",
    "Y_predicted=ridge_regression.predict(W=W_learned, X_new=X_test)\n",
    "print (ridge_regression.compute_RSS(Y_new=Y_test, Y_predicted=Y_predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
